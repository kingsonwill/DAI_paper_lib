## DML
1. **COLA: Decentralized Linear Learning**
    - http://papers.nips.cc/paper/7705-cola-decentralized-linear-learning
2. **Distributed Learning without Distress: Privacy-Preserving Empirical Risk Minimization**
    - http://papers.nips.cc/paper/7871-distributed-learning-without-distress-privacy-preserving-empirical-risk-minimization
3. **BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training**
    - http://papers.nips.cc/paper/7678-bml-a-high-performance-low-cost-gradient-synchronization-algorithm-for-dml-training
4. **Distributed Stochastic Optimization via Adaptive SGD**
    - http://papers.nips.cc/paper/7461-distributed-stochastic-optimization-via-adaptive-sgd
5. **LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning**
    - http://papers.nips.cc/paper/7752-lag-lazily-aggregated-gradient-for-communication-efficient-distributed-learning
6. **cpSGD: Communication-efficient and differentially-private distributed SGD**
    - http://papers.nips.cc/paper/7984-cpsgd-communication-efficient-and-differentially-private-distributed-sgd
7. **Distributed Weight Consolidation: A Brain Segmentation Case Study**
    - http://papers.nips.cc/paper/7664-distributed-weight-consolidation-a-brain-segmentation-case-study
8. **Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training**
    - http://papers.nips.cc/paper/8028-pipe-sgd-a-decentralized-pipelined-sgd-framework-for-distributed-deep-net-training
9. **Communication Compression for Decentralized Training**
    - http://papers.nips.cc/paper/7992-communication-compression-for-decentralized-training 
10. **Bayesian Distributed Stochastic Gradient Descent**
    - http://papers.nips.cc/paper/7874-bayesian-distributed-stochastic-gradient-descent 
11. **Byzantine Stochastic Gradient Descent**
    - http://papers.nips.cc/paper/7712-byzantine-stochastic-gradient-descent 
12. **Distributed k-Clustering for Data with Heavy Noise**
    - http://papers.nips.cc/paper/8009-distributed-k-clustering-for-data-with-heavy-noise 
13. **Proximal SCOPE for Distributed Sparse Learning**
    - http://papers.nips.cc/paper/7890-proximal-scope-for-distributed-sparse-learning
14. **Snap ML: A Hierarchical Framework for Machine Learning**
    - http://papers.nips.cc/paper/7309-snap-ml-a-hierarchical-framework-for-machine-learning
15. **Decentralize and Randomize: Faster Algorithm for Wasserstein Barycenters**
    - http://papers.nips.cc/paper/8274-decentralize-and-randomize-faster-algorithm-for-wasserstein-bar


## optimization

1. **Gradient Sparsification for Communication-Efficient Distributed Optimization**
    - http://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization
2. **GIANT: Globally Improved Approximate Newton Method for Distributed Optimization**
    - http://papers.nips.cc/paper/7501-giant-globally-improved-approximate-newton-method-for-distributed-optimization
3. **A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication**
    - http://papers.nips.cc/paper/7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication
4. **New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity**
    - http://papers.nips.cc/paper/7399-new-insight-into-hybrid-stochastic-gradient-descent-beyond-with-replacement-sampling-and-convexity
5. **Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks**
    - http://papers.nips.cc/paper/7844-evolutionary-stochastic-gradient-descent-for-optimization-of-deep-neural-networks
6. **How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective**
    - http://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective
7. **Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization**
    - http://papers.nips.cc/paper/7626-towards-understanding-acceleration-tradeoff-between-momentum-and-asynchrony-in-nonconvex-stochastic-optimization
8. **Stochastic Primal-Dual Method for Empirical Risk Minimization with O(1) Per-Iteration Complexity**
    - http://papers.nips.cc/paper/8057-stochastic-primal-dual-method-for-empirical-risk-minimization-with-o1-per-iteration-complexity


## Privacy Preserving & price

1. **Model-Agnostic Private Learning**
    - http://papers.nips.cc/paper/7941-model-agnostic-private-learning
2. **Contamination Attacks and Mitigation in Multi-Party Machine Learning**
    - http://papers.nips.cc/paper/7895-contamination-attacks-and-mitigation-in-multi-party-machine-learning
3. **Local Differential Privacy for Evolving Data**
    - http://papers.nips.cc/paper/7505-local-differential-privacy-for-evolving-data
4. **Re-evaluating evaluation**
    - http://papers.nips.cc/paper/7588-re-evaluating-evaluation
5. **Differentially Private k-Means with Constant Multiplicative Error**
    - http://papers.nips.cc/paper/7788-differentially-private-k-means-with-constant-multiplicative-error
6. **Differentially Private Testing of Identity and Closeness of Discrete Distributions**
    - http://papers.nips.cc/paper/7920-differentially-private-testing-of-identity-and-closeness-of-discrete-distributions
7. **Differential Privacy for Growing Databases**
    - http://papers.nips.cc/paper/8102-differential-privacy-for-growing-databases
8. **Differentially Private Robust Low-Rank Approximation**
    - http://papers.nips.cc/paper/7668-differentially-private-robust-low-rank-approximation
9. **Learning Attentional Communication for Multi-Agent Cooperation**
    - http://papers.nips.cc/paper/7956-learning-attentional-communication-for-multi-agent-cooperation
10. **Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions**
    - http://papers.nips.cc/paper/8282-domain-adaptation-by-using-causal-inference-to-predict-invariant-conditional-distributions
11. **Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise**
    - http://papers.nips.cc/paper/8246-using-trusted-data-to-train-deep-networks-on-labels-corrupted-by-severe-noise
12. **Query Complexity of Bayesian Private Learning**
    - http://papers.nips.cc/paper/7510-query-complexity-of-bayesian-private-learning
13. **Adaptive Learning with Unknown Information Flows**
    - http://papers.nips.cc/paper/7976-adaptive-learning-with-unknown-information-flows
14. **Differentially Private Uniformly Most Powerful Tests for Binomial Data**
    - http://papers.nips.cc/paper/7675-differentially-private-uniformly-most-powerful-tests-for-binomial-data
15. **The Price of Privacy for Low-rank Factorization**
    - http://papers.nips.cc/paper/7672-the-price-of-privacy-for-low-rank-factorization


## other 
(maybe useful)

1. https://ppml-workshop.github.io/ppml/
2. **Isolating Sources of Disentanglement in Variational Autoencoders**
    - http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders
3. **The Nearest Neighbor Information Estimator is Adaptively Near Minimax Rate-Optimal**
    - http://papers.nips.cc/paper/7578-the-nearest-neighbor-information-estimator-is-adaptively-near-minimax-rate-optimal
4. **The Everlasting Database: Statistical Validity at a Fair Price**
    - http://papers.nips.cc/paper/7888-the-everlasting-database-statistical-validity-at-a-fair-price
5. **Coordinate Descent with Bandit Sampling**
    - http://papers.nips.cc/paper/8137-coordinate-descent-with-bandit-sampling
6. **Neighbourhood Consensus Networks**
    - http://papers.nips.cc/paper/7437-neighbourhood-consensus-networks
7. **Exact natural gradient in deep linear networks and its application to the nonlinear case**
    - http://papers.nips.cc/paper/7834-exact-natural-gradient-in-deep-linear-networks-and-its-application-to-the-nonlinear-case
8. **GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training**
    - http://papers.nips.cc/paper/7759-gradiveq-vector-quantization-for-bandwidth-efficient-gradient-aggregation-in-distributed-cnn-training